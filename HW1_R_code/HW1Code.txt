---
title: "HW1 Econometrics 3"
author: "Matthew Aaron Looney"
date: "10/04/2017"
output: pdf_document
---

```{r HW1 Problem 1 part a and b, echo=FALSE, message=FALSE, results='asis', cache=F}

cat("\014")
rm(list=ls())
set.seed(12345)

 library(readr)
library(gvlma)
library(lmtest)
library(stargazer)

data <- read_csv("~/Google Drive/digitalLibrary/*Econometrics3/Econometrics3/HW1_MatLab_Code/data.csv", col_names = FALSE)

n=20 # sample size
k=3 # number of paramaters in model
reps <- 1e2 # number of ols iterations
i <- 1 # counter
temp <- matrix(nrow = reps, ncol = 14) # matrix to stuff results into dim= "reps" X k+3
x1 <- data$X2
x2 <- data$X3

for(i in 1:reps){
  
#e <- rnorm(n=n, mean=0, sd=5) # n-random normal draws for error term
#x1 <- rnorm(n=n, mean=100, sd=7) # n-random normal draws for x1 data
#x2 <- rnorm(n=n, mean=1000, sd=10) # n-random normal draws for x2 data
e= sqrt(exp(-2+ 0.25* x1)) *rnorm(n)

y <- 10+ 1* x1+ 1* x2 +e # construct model

X <- matrix(c(rep(1, n), x1, x2), nrow=n, ncol=k) # gather X-data into single matrix

b_ols <- solve(crossprod(X, X)) %*% crossprod(X, y) # faster to use matrix algebra over lm function

ehat <- y-(X %*% b_ols)
sigma2 <- (t(ehat) %*% ehat) / (n-k)
sigma2 <- as.numeric(sigma2)
covb <- diag(sigma2 * solve(crossprod(X, X)))

bp_ols <- lm(y~x1+x2)
bp_test <- bptest(bp_ols, studentize = F)
gvHet_test <- gvlma(bp_ols)

temp[i, 1] <- b_ols[1]
temp[i, 2] <- b_ols[2]
temp[i, 3] <- b_ols[3]
temp[i, 4] <- covb[1]
temp[i, 5] <- covb[2]
temp[i, 6] <- covb[3]
temp[i, 7] <- sqrt(covb[1])
temp[i, 8] <- sqrt(covb[2])
temp[i, 9] <- sqrt(covb[3])
temp[i, 10] <- mean(b_ols[1]/sqrt(covb[1]))
temp[i, 11] <- mean(b_ols[2]/sqrt(covb[2]))
temp[i, 12] <- mean(b_ols[3]/sqrt(covb[3]))
temp[i, 13] <- unlist(bp_test[1])
temp[i, 14] <- gvHet_test$GlobalTest$DirectionalStat4$Value

i <- i+1
}
  
dimnames(temp) <- list(c(), c("beta_0_ols", "beta_1_ols", "beta_2_ols", "var_b0_ols", "var_b1_ols", "var_b2_ols", "se_b0_ols", "se_b1_ols", "se_b2_ols", "t_val_b0_ols", "t_val_b1_ols", "t_val_b2_ols", "BP_testStat_ols", "GV_HET_Test_ols"))
  
# ols_params_mu <- matrix(nrow=3, ncol=3)
# 
# ols_params_mu[1,1] <- mean(temp[,1])
# ols_params_mu[2,1] <- mean(temp[,2])
# ols_params_mu[3,1] <- mean(temp[,3])
# 
# ols_params_mu[1,2] <- sqrt(mean(temp[, 4]))
# ols_params_mu[2,2] <- sqrt(mean(temp[, 5]))
# ols_params_mu[3,2] <- sqrt(mean(temp[, 6]))
# 
# ols_params_mu[1,3] <- mean(temp[,1])/ sqrt(mean(temp[, 4]))
# ols_params_mu[2,3] <- mean(temp[,2])/ sqrt(mean(temp[, 5]))
# ols_params_mu[3,3] <- mean(temp[,3])/ sqrt(mean(temp[, 6]))
# 
# dimnames(ols_params_mu) <- list(c(), c("betas_ols", "se_ols", "t_values_ols"))
# ols_params_mu

#ehat <- y-(X %*% b_ols)
# sigma2 <- (t(ehat) %*% ehat) / (n-k)
# sigma2 <- as.numeric(sigma2)
# covb <- sigma2 * solve(crossprod(X, X))
# se_b <- sqrt(diag(covb))
# t_val <- mu_betas/se_b
# dimnames(t_val) <- list(c(), c("t_values_ols"))
# t_val

```
  
The Model:  
  
\begin{equation}
{y_t} = {\beta _0} + {\beta _1}{x_1} + {\beta _2}{x_2} + {\varepsilon _t}
\end{equation}
  
where,  
  
$\varepsilon _t$ is normally and independently distributed with $E[\varepsilon _t]=0,$ $E[\varepsilon _t^2]=\sigma^2_t$ and $\sigma^2_t=exp(\alpha_0+\alpha_1x_1)$.  
  
The parameter values used are $\beta_0=10,$ $\beta_1=\beta_2=1,$ $\alpha_0=-2,$ and $\alpha_1=0.25$.  
  
The design matrix $X$ is given in a csv file.  
  
# Problem 1  
  
The following questions are based on Monte Carlo experimental data. Generate 100 samples of $y$, each of size 20, using the model given in equation 1 (above).


## (1.a)  
  
Estimate the parameters using the least squares principle and provide their covariance matrix. Compare your results with the true parameters. What can you conclude?  
  
```{r, echo=FALSE, message=FALSE, results='asis', fig.height=3, results='asis'}

stargazer(temp, header=F, type="latex", summary = T, font.size = "small", notes= c("BP testStat ols = Breusch Pagan test against heteroskedasticity", "GV HET Test ols = General test for LM assumptions", "Chi Squared Crit df:2 at alpha:0.05 = 5.99"), notes.align= "l", flip = F, float = T, float.env = "table", title="Summary Statistics: Problem 1, part (a) and (b)", median = F)

 # par(mfrow=c(1,3))
 # hist(temp[,1])
 # hist(temp[,2])
 # hist(temp[,3])

```
  
The results displayed in table 1 (above) show the summary for the 100 sample experiments preformed on the data using OLS. The means of the 100 samples are reported for various statistics. The average t-values for the beta parameters show the constant is insignificant while $\beta_1$ and $\beta_2$ are significant. To better understand the distribution of the parameters over the 100 experiments see density plots below.  
  
```{r, echo=FALSE, message=FALSE, results='asis', fig.height=3, results='asis'}

par(mfrow=c(1,3))
plot(density(temp[,1]), main= expression(paste("ols-", beta[0], sep="")))
plot(density(temp[,2]), main= expression(paste("ols-", beta[1], sep="")))
plot(density(temp[,3]), main= expression(paste("ols-", beta[2], sep="")))

```
  
## (1.b)  
  
Test for the presence of heteroskedasticity? What do you conclude?  
  
I have run the Breusch Pagan (BP) test for heteroskedasticity for the 100 samples and averaged over the experiments. The BP test statistic is distributed as Chi-Squared with k-1 degrees of freedom, where k is the total number of parameters in the model.  
  
The null hypothesis of the Breusch-Pagan test is,

$$\sigma _i^2 = {\sigma ^2}({\alpha _0} + \alpha '{z_i})$$
where,  
  
${H_0}:\alpha  = 0$
  
$\sigma _i^2$ is the error variance for the ith observation and $\alpha_0$ and $\alpha$ are regression coefficients.  
  
The test statistic for the Breusch-Pagan test is:  
  
\begin{equation}
bp = {1 \over v}(u - \bar ui)'Z{(Z'Z)^{ - 1}}Z'(u - \bar ui)
\end{equation}
  
where $u = (e_1^2,e_2^2,...,e_n^2)$, $i$ is a $nx1$ vector of ones, and

$$v = {1 \over n}{\sum\limits_{i = 1}^n {\left( {e_i^2 - {{e'e} \over n}} \right)} ^2}$$
  
The result from any one of the 100 experiments may or may not indicate heteroskedasticity, however, when we average over the range of the 100 experiments we obtain a test statistic which would suggest, overall we have a heteroskedasticity problem.   
  
**This is a modified version of the Breusch-Pagan test, which is less sensitive to the assumption of normality than the original test (Greene 2012, 7th Ed.; p. 276).**
  
  
```{r HW1 Problem 1 part c, echo=FALSE, message=FALSE, results='asis', fig.height=3}

cat("\014")
rm(list=ls())
set.seed(12345)

library(readr)
library(car)

data <- read_csv("~/Google Drive/digitalLibrary/*Econometrics3/Econometrics3/HW1_MatLab_Code/data.csv", col_names = FALSE)
x1 <- data$X2
x2 <- data$X3

n=20 # sample size
k=3 # number of paramaters in model
reps <- 1e2 # number of ols iterations
i <- 1 # counter
temp <- matrix(nrow = reps, ncol = 32) # matrix to stuff results into dim= "reps" X k+3

for(i in 1:reps){
  
#e <- rnorm(n=n, mean=0, sd=5) # n-random normal draws for error term
#x1 <- rnorm(n=n, mean=100, sd=7) # n-random normal draws for x1 data
#x2 <- rnorm(n=n, mean=1000, sd=10) # n-random normal draws for x2 data
e= sqrt(exp(-2+ 0.25* x1)) *rnorm(n)

y <- matrix(10+ 1* x1+ 1* x2 +e) # construct model

X <- matrix(c(rep(1, n), x1, x2), nrow=n, ncol=k) # gather X-data into single matrix

b_ols <- solve( crossprod(X, X) ) %*% crossprod(X, y) # faster to use matrix algebra over lm function

ehat <- y-(X %*% b_ols)
sigma2 <- (t(ehat) %*% ehat) / (n-k)
sigma2 <- as.numeric(sigma2)
covb <- diag(sigma2 * solve(crossprod(X, X)))

### GLS section ###

ehat2 <- ehat^2
# w_gls <- solve( crossprod(X[,-3], X[,-3]) ) %*% crossprod(X[,-3], log(ehat2))
# w_gls_hat <- exp(w_gls[1]+ w_gls[2]* x1+ w_gls[3]* x2)
# w_gls_hat_sq <- diag(w_gls_hat)
# w_gls_hat_inv <- solve(w_gls_hat_sq)
w_gls <- solve( crossprod(X[,-3], X[,-3]) ) %*% crossprod(X[,-3], log(ehat2))
w_gls_hat <- exp(w_gls[1]+ w_gls[2]* x1)
w_gls_hat_sq <- diag(w_gls_hat)
w_gls_hat_inv <- solve(w_gls_hat_sq)

b_ols_fgls <- solve( t(X) %*% w_gls_hat_inv %*% X ) %*% ( t(X) %*% w_gls_hat_inv %*% y )
#te <- lm(y~ x1+x2, weights=w_gls_hat^-1)

#vcov_ols_fgls <- sigma2*solve(t(X)%*% w_gls_hat_inv%*% X)
# pg 270 greene, 7th Ed.
vcov_ols_fgls <- sigma2 * (solve( crossprod(X, X) ) %*% ( t(X) %*% w_gls_hat_inv %*% X ) %*% solve(crossprod(X, X)) )

vcov_ols_fgls_diag <- diag(vcov_ols_fgls)

te1 <- lm(y~ x1+ x2)
vcovHCCM0 <- diag(hccm(te1, type="hc0"))
vcovHCCM3 <- diag(hccm(te1, type="hc3"))

temp[i, 1] <- b_ols[1]
temp[i, 2] <- b_ols[2]
temp[i, 3] <- b_ols[3]
temp[i, 4] <- covb[1]
temp[i, 5] <- covb[2]
temp[i, 6] <- covb[3]
temp[i, 7] <- sqrt(covb[1])
temp[i, 8] <- sqrt(covb[2])
temp[i, 9] <- sqrt(covb[3])
temp[i, 10] <- mean(b_ols[1]/sqrt(covb[1]))
temp[i, 11] <- mean(b_ols[2]/sqrt(covb[2]))
temp[i, 12] <- mean(b_ols[3]/sqrt(covb[3]))
temp[i, 13] <- b_ols_fgls[1]
temp[i, 14] <- b_ols_fgls[2]
temp[i, 15] <- b_ols_fgls[3]

temp[i, 16] <- w_gls[1]
temp[i, 17] <- w_gls[2]

temp[i, 18] <- vcov_ols_fgls_diag[1]
temp[i, 19] <- vcov_ols_fgls_diag[2]
temp[i, 20] <- vcov_ols_fgls_diag[3]

temp[i, 21] <- sqrt(vcov_ols_fgls_diag[1])
temp[i, 22] <- sqrt(vcov_ols_fgls_diag[2])
temp[i, 23] <- sqrt(vcov_ols_fgls_diag[3])
temp[i, 24] <- (b_ols_fgls[1])/sqrt(vcov_ols_fgls_diag[1])
temp[i, 25] <- (b_ols_fgls[2])/sqrt(vcov_ols_fgls_diag[2])
temp[i, 26] <- (b_ols_fgls[3])/sqrt(vcov_ols_fgls_diag[3])
temp[i, 27] <- vcovHCCM0[1]
temp[i, 28] <- vcovHCCM0[2]
temp[i, 29] <- vcovHCCM0[3]
temp[i, 30] <- vcovHCCM3[1]
temp[i, 31] <- vcovHCCM3[2]
temp[i, 32] <- vcovHCCM3[3]

i <- i+1
}
  
dimnames(temp) <- list(c(), c("beta_0_ols", "beta_1_ols", "beta_2_ols", "var_b0_ols", "var_b1_ols", "var_b2_ols", "se_b0_ols", "se_b1_ols", "se_b2_ols", "t_val_b0_ols", "t_val_b1_ols", "t_val_b2_ols", "beta_0_fgls", "beta_1_fgls", "beta_2_fgls", "alpha_0_fgls", "alpha_1_fgls", "var_b0_fgls", "var_b1_fgls", "var_b2_fgls", "se_b0_fgls", "se_b1_fgls", "se_b2_fgls", "t_val_b0_fgls", "t_val_b1_fgls", "t_val_b2_fgls", "var_b0_HCCM_0", "var_b1_HCCM_0", "var_b2_HCCM_0", "var_b0_HCCM_3", "var_b1_HCCM_3", "var_b2_HCCM_3"))
  
# ols_params_mu <- matrix(nrow=3, ncol=2)
# ols_params_mu[1,1] <- mean(temp[,1])
# ols_params_mu[2,1] <- mean(temp[,2])
# ols_params_mu[3,1] <- mean(temp[,3])
# ols_params_mu[1,2] <- mean(temp[,1])/ sqrt(mean(temp[, 4]))
# ols_params_mu[2,2] <- mean(temp[,2])/ sqrt(mean(temp[, 5]))
# ols_params_mu[3,2] <- mean(temp[,3])/ sqrt(mean(temp[, 6]))
# 
# dimnames(ols_params_mu) <- list(c(), c("betas_ols", "t_values_ols"))
# ols_params_mu

# ehat <- y-(X %*% b_ols)
# sigma2 <- (t(ehat) %*% ehat) / (n-k)
# sigma2 <- as.numeric(sigma2)
# covb <- sigma2 * solve(crossprod(X, X))
# se_b <- sqrt(diag(covb))
# t_val <- mu_betas/se_b
# dimnames(t_val) <- list(c(), c("t_values_ols"))
# t_val

```
  
\newpage  
  
## (1.c)  
  
Assuming multiplicative heteroskedasticity, estimate the parameters using GLS and provide their covariance matrix.
  
Table 2 (below) shows the summary of results obtained using Feasible generalized least squares (FGLS) to obtain an estimate of the variance-covariance structure. Once we have this structure ($\hat \Omega^{-1}$) in hand we can proceed with estimation of the Beta's and the correct variance-covariance.  
  
When comparing the FGLS results with OLS we notice the average of the parameter estimates are similar enough to be considered the same, as we expect. However, we observe the variance-covariance of the FGLS is much smaller, indicating a much better job at recovering the nature of the error structure and addressing the heteroskedasticity problem embedded in the data. Since we have generated the data with known structure and parameters we can verify the accuracy of our techniques ability to uncover the true parameters. The FGLS technique does an excellent job at recovering both the beta's of our data and the alpha parameters of our error structure. Using the FGLS technique we also notice all of our parameter estimates are significant at the 95% level, a change from the OLS procedure where only two of the three parameters were significant.  
  
To allow us to understand the distribution of the FGLS parameter estimates over the range of the 100 experiments, I provide density plot's (below).  
  
```{r, echo=FALSE, message=FALSE, results='asis', fig.height=3, results='asis'}

stargazer(temp, header=F, type="latex", summary = T, font.size = "small", notes= c(), notes.align= "l", flip = F, float = T, float.env = "table", title="Summary Statistics: Problem 1, part (c)", median = F)

```
  
```{r, echo=FALSE, message=FALSE, results='asis', fig.height=3, results='asis'}

par(mfrow=c(1,3))
plot(density(temp[,13]), main= expression(paste("FGLS-", beta[0], sep="")))
plot(density(temp[,14]), main= expression(paste("FGLS-", beta[1], sep="")))
plot(density(temp[,15]), main= expression(paste("FGLS-", beta[2], sep="")))
par(mfrow=c(1,2))
plot(density(temp[,16]), main= expression(paste("FGLS-", alpha[0], sep="")))
plot(density(temp[,17]), main= expression(paste("FGLS-", alpha[1], sep="")))

```
  
```{r HW1 Problem 1 part d, echo=FALSE, message=FALSE, results='asis', fig.height=3, warning=FALSE}

###################
### MLE Section ###
###################

### housekeeping ###

cat("\014")
rm(list=ls())
set.seed(12345)

 library(readr)

data <- read_csv("~/Google Drive/digitalLibrary/*Econometrics3/Econometrics3/HW1_MatLab_Code/data.csv", col_names = FALSE)

n=20 # sample size
k=3 # number of paramaters in model
reps <- 1e2 # number of ols iterations
i <- 1 # counter
temp <- matrix(nrow = reps, ncol = 20) # matrix to stuff results into dim= "reps" X 3
x1 <- data$X2
x2 <- data$X3

### MLE functions ###

hw1.loglike <- function(params, y, x1, x2)
{
  beta1 <- params[1]
  beta2 <- params[2]
  beta3 <- params[3]
  beta4 <- params[4]
  beta5 <- params[5]
  
  n <- length(x1)
    
    lnL= -0.5* n* log(2* pi)- 0.5* sum(beta4+ beta5* x1)- 0.5* sum( ( (y- (beta1+ beta2* x1+ beta3* x2))^2 )/ ( exp(beta4+ beta5* x1) ) ) 
  
  return(lnL)
}

hw1.mle <- function( par0, y, x1, x2)
{
  
   temp.mle <- optim(par0, hw1.loglike, y=y, x1=x1, x2=x2, method="Nelder-Mead", 
                     control=list(fnscale= -1, maxit = 30000, reltol=sqrt(.Machine$double.eps)^2), hessian= T)
  
#   temp.mle <- ucminf(par0, hw1.loglike, y=y, x1=x1, x2=x2)
  
   if(temp.mle$convergence!=0)
     stop("!!!DID NOT CONVERGE!!!")
  
  return(temp.mle)
}

### get starting values for MLE; standard OLS estimation ###

for(i in 1:reps){
  
#e <- rnorm(n=n, mean=0, sd=5) # n-random normal draws for error term
#x1 <- rnorm(n=n, mean=100, sd=7) # n-random normal draws for x1 data
#x2 <- rnorm(n=n, mean=1000, sd=10) # n-random normal draws for x2 data

e= sqrt(exp(-2+ 0.25* x1)) *rnorm(n)

y <- 10+ 1* x1+ 1* x2 +e # construct model

X <- matrix(c(rep(1, n), x1, x2), nrow=n, ncol=k) # gather X-data into single matrix

b_ols <- solve(crossprod(X, X)) %*% crossprod(X, y) # faster to use matrix algebra over lm function

##################################################################
### MLE estimation ###

par0= c(b_ols[1], b_ols[2], b_ols[3], 0, 0) # obtain starting values from ols

#hw1.loglike(params=par0, y=y, x1=x1, x2=x2)

beta_mle <- hw1.mle(par0=par0, y=y, x1=x1, x2=x2) # run MLE precedure
#beta_mle1 <- nlm(hw1.loglike, p=par0, y=y, x1=x1, x2=x2) # run MLE precedure

beta_mle_params <- matrix(beta_mle$par, nrow=5)
var_mle <- -(1)* diag(solve(beta_mle$hessian))

dimnames(beta_mle_params) <- list(c(), c("betas_mle"))
temp[i, 1] <- beta_mle_params[1]
temp[i, 2] <- beta_mle_params[2]
temp[i, 3] <- beta_mle_params[3] 
temp[i, 4] <- beta_mle_params[4] 
temp[i, 5] <- beta_mle_params[5] 
temp[i, 6] <- var_mle[1]
temp[i, 7] <- var_mle[2]
temp[i, 8] <- var_mle[3]
temp[i, 9] <- var_mle[4]
temp[i, 10] <- var_mle[5]

temp[i, 11] <- sqrt(var_mle[1])
temp[i, 12] <- sqrt(var_mle[2])
temp[i, 13] <- sqrt(var_mle[3])
temp[i, 14] <- sqrt(var_mle[4])
temp[i, 15] <- sqrt(var_mle[5])

temp[i, 16] <- mean(beta_mle_params[1]/sqrt(var_mle[1]))
temp[i, 17] <- mean(beta_mle_params[2]/sqrt(var_mle[2]))
temp[i, 18] <- mean(beta_mle_params[3]/sqrt(var_mle[3]))
temp[i, 19] <- mean(beta_mle_params[4]/sqrt(var_mle[4]))
temp[i, 20] <- mean(beta_mle_params[5]/sqrt(var_mle[5]))

#mle_se <- sqrt(var_mle)

#mle_t_val <- beta_mle$par / mle_se
#mle_t_val <- matrix(mle_t_val, nrow=5)
#dimnames(mle_t_val) <- list(c(), c("t_values__mle"))
#mle_t_val
i <- i+1
}

dimnames(temp) <- list(c(), c("beta_0_mle", "beta_1_mle", "beta_2_mle", "alpha_0_mle", "alpha_1_mle", "var_b0_mle", "var_b1_mle", "var_b2_mle", "var_alpha_0_mle", "var_alpha_1_mle", "se_b0_mle", "se_b1_mle", "se_b2_mle", "se_alpha_0_mle", "se_alpha_1_mle", "t_val_b0_mle", "t_val_b1_mle", "t_val_b2_mle", "t_val_alpha_0_mle", "t_val_alpha_1_mle"))

```
  
## (1.d)  
  
Write a Matlab code to estimate the parameters using maximum likelihood method and provide their covariance matrix.   
\begin{equation}
\ln L =  - 0.5n\log (2\pi ) - 0.5\sum\limits_{}^{} {({\sigma ^2}) - 0.5\sum\limits_{}^{} {\left[ {\frac{{{{(y - X'\beta )}^2}}}{{{\sigma ^2}}}} \right]} }
\end{equation}
  
where,  

${\sigma ^2} \simeq \exp ({\alpha _0} + {\alpha _1}{x_1})$
  
Table 3 (below) gives the summary statistics for the Maximum Likelihood Estimation method (MLE). Again we notice the MLE does a good job at uncovering the true parameters of our model. The MLE has the added benefit of obtaining all parameter estimates at one time, assuming the MLE assumptions are met.  
  
When we compare the results obtained using the MLE procedure we notice all parameter estimates are similar, however, when we view the variance-covariance matrix we see that the FGLS seems to do a better job over the MLE procedure. This may be due to the optimization routine used for the MLE or the convergence tolerance, or both. More experimenting with the optimization variables could yield better results. At any rate the MLE estimation strategy does a better job that OLS and just about as good a job as FGLS, in the current context.  
  
To help us better understand the distribution of the MLE parameter estimates over the range of the 100 experiments, I provide density plot's (below).  
  
```{r, echo=FALSE, message=FALSE, results='asis', fig.height=3, results='asis'}

stargazer(temp, header=F, type="latex", summary = T, font.size = "small", notes= c(), notes.align= "l", flip = F, float = T, float.env = "table", title="Summary Statistics: Problem 1, part (d)", median = F)

```
  
```{r, echo=FALSE, message=FALSE, results='asis', fig.height=3, results='asis'}

par(mfrow=c(1,3))
plot(density(temp[,1]), main= expression(paste("MLE-", beta[0], sep="")))
plot(density(temp[,2]), main= expression(paste("MLE-", beta[1], sep="")))
plot(density(temp[,3]), main= expression(paste("MLE-", beta[2], sep="")))
par(mfrow=c(1,2))
plot(density(temp[,4]), main= expression(paste("MLE-", alpha[0], sep="")))
plot(density(temp[,5]), main= expression(paste("MLE-", alpha[1], sep="")))

```
  
```{r HW1 Problem 2 part a, echo=FALSE, message=FALSE, results='asis', fig.height=3, warning=FALSE}

###################
### OLS ###
###################

### housekeeping ###

cat("\014")
rm(list=ls())
set.seed(12345)

 library(readr)
library(lmtest)
library(car)

# Cochrane–Orcutt procedure
# C.O. funcition requires Y, X (with constant), OLS b.
c.o.proc <- function(Y,X,b_0,tol){
  T <- length(y)
  e <- y - (X%*%b_0) #OLS residuals
  rss <- sum(e^2); 
  rss_1 <- rss; 
  d_rss = rss
  e2 <- e[-1]; 
  e3 <- e[-T]
  ols_e0 <- lm(e2~e3-1)
  rho <- ols_e0$coeff[1] #initial value for rho
  i<-1
  while (d_rss > tol) {
    rss <- rss_1 # RSS at iter (i-1)
    YY <- y[2:T] - rho * y[1:(T-1)] # pseudo-diff Y
    XX <- X[2:T, ] - rho * X[1:(T-1), ] # pseudo-diff X
    ols_yx <- lm(YY~XX-1) # adjust if constant
    b <- ols_yx$coef # updated b at iter i
    # b[1] <- b[1]/(1-rho) #If constant not pseudo-diff
    e1 <- y - X%*%b # update residuals
    e2 <- e1[-1]
    e3 <- e1[-T]
    ols_e1 <- lm(e2~e3-1)
    rho <- ols_e1$coeff[1] # rho i
    rss_1 <- sum(e1^2) # RSS i
    d_rss <- abs(rss_1-rss) # diff in RSS
    i <- i+1
  }
  result <-list()
  result$Cochrane_Orc.Proc <- summary(ols_yx)
  result$rho.regression <- summary(ols_e1)
  # result$Corrected.b_1 <- b[1]
  result$Iterations <-i-1
  return(result)
}

################################################################################

autocorr.mat <- function(p = 20, rho = 0.8) {
  mat <- diag(p)
  return(rho^abs(row(mat)-col(mat)))
}

################################################################################

data <- read_csv("~/Google Drive/digitalLibrary/*Econometrics3/Econometrics3/HW1_MatLab_Code/data.csv", col_names = FALSE)

n=20 # sample size
k=3 # number of paramaters in model
reps <- 1e2 # number of ols iterations
i <- 1 # counter
temp <- matrix(nrow = reps, ncol = 34) # matrix to stuff results into dim= "reps" X 3
x1 <- data$X2
x2 <- data$X3

for(i in 1:reps){
  
et <- arima.sim(n = 20, list(ar = c(-0.8)), sd = sqrt(6.4))
et <- matrix(et)
#et= sqrt(exp(-2+ 0.25* x1)) *rnorm(n)

y <- 10+ 1* x1+ 1* x2+ et # construct model

X <- matrix(c(rep(1, n), x1, x2), nrow=n, ncol=k) # gather X-data into single matrix

b_ols <- solve(crossprod(X, X)) %*% crossprod(X, y) # faster to use matrix algebra over lm function

ehat <- y-(X %*% b_ols)
sigma2 <- (t(ehat) %*% ehat) / (n-k)
sigma2 <- as.numeric(sigma2)
covb <- diag(sigma2 * solve(crossprod(X, X)))

DW_ols <- lm(y~x1+x2)
DW_test <- dwtest(DW_ols)
bp_test <- bptest(DW_ols)
#DW_test

# ehat_lag <- ehat[-1]
# ehat_lag1 <- ehat[-20]
# lm(ehat_lag~ ehat_lag1-1)

# using Cochrane–Orcutt procedure
rho_est <- c.o.proc(Y=y, X=X, b_0=b_ols, tol = 1e-12)

rho_hat_fgls <- -rho_est$rho.regression$coefficients[1]
sigma2_hat_fgls <- rho_est$Cochrane_Orc.Proc$sigma^2

omega_hat <- sigma2_hat_fgls* autocorr.mat(p=n, rho= rho_hat_fgls)
omega_hat_inv <- solve(omega_hat)

b_ols_fgls <- solve( t(X) %*% omega_hat_inv %*% X ) %*% ( t(X) %*% omega_hat_inv %*% y )

# pg 270 greene, 7th Ed.
vcov_ols_fgls <- sigma2 * (solve( crossprod(X, X) ) %*% ( t(X) %*% omega_hat_inv %*% X ) %*% solve(crossprod(X, X)) )

vcov_ols_fgls_diag <- diag(vcov_ols_fgls)

te1 <- lm(y~ x1+ x2)
vcovHCCM0 <- diag(hccm(te1, type="hc0"))
vcovHCCM3 <- diag(hccm(te1, type="hc3"))

temp[i, 1] <- b_ols[1]
temp[i, 2] <- b_ols[2]
temp[i, 3] <- b_ols[3]
temp[i, 4] <- covb[1]
temp[i, 5] <- covb[2]
temp[i, 6] <- covb[3]
temp[i, 7] <- sqrt(covb[1])
temp[i, 8] <- sqrt(covb[2])
temp[i, 9] <- sqrt(covb[3])
temp[i, 10] <- mean(b_ols[1]/sqrt(covb[1]))
temp[i, 11] <- mean(b_ols[2]/sqrt(covb[2]))
temp[i, 12] <- mean(b_ols[3]/sqrt(covb[3]))

temp[i, 13] <- DW_test$statistic
temp[i, 14] <- bp_test$statistic

temp[i, 15] <- rho_hat_fgls
temp[i, 16] <- sigma2_hat_fgls
temp[i, 17] <- b_ols_fgls[1]
temp[i, 18] <- b_ols_fgls[2]
temp[i, 19] <- b_ols_fgls[3]

temp[i, 20] <- vcov_ols_fgls_diag[1]
temp[i, 21] <- vcov_ols_fgls_diag[2]
temp[i, 22] <- vcov_ols_fgls_diag[3]

temp[i, 23] <- sqrt(vcov_ols_fgls_diag[1])
temp[i, 24] <- sqrt(vcov_ols_fgls_diag[2])
temp[i, 25] <- sqrt(vcov_ols_fgls_diag[3])
temp[i, 26] <- (b_ols_fgls[1])/sqrt(vcov_ols_fgls_diag[1])
temp[i, 27] <- (b_ols_fgls[2])/sqrt(vcov_ols_fgls_diag[2])
temp[i, 28] <- (b_ols_fgls[3])/sqrt(vcov_ols_fgls_diag[3])

temp[i, 29] <- vcovHCCM0[1]
temp[i, 30] <- vcovHCCM0[2]
temp[i, 31] <- vcovHCCM0[3]
temp[i, 32] <- vcovHCCM3[1]
temp[i, 33] <- vcovHCCM3[2]
temp[i, 34] <- vcovHCCM3[3]


i <- i+1
}

dimnames(temp) <- list(c(), c("beta_0_ols", "beta_1_ols", "beta_2_ols", "var_b0_ols", "var_b1_ols", "var_b2_ols", "se_b0_ols", "se_b1_ols", "se_b2_ols", "t_val_b0_ols", "t_val_b1_ols", "t_val_b2_ols", "DW_Test", "BP_Test", "rho_fgls", "sigma2_fgls", "beta_0_fgls", "beta_1_fgls", "beta_2_fgls", "var_b0_fgls", "var_b1_fgls", "var_b2_fgls", "se_b0_fgls", "se_b1_fgls", "se_b2_fgls", "t_val_b0_fgls", "t_val_b1_fgls", "t_val_b2_fgls", "var_b0_HCCM_0", "var_b1_HCCM_0", "var_b2_HCCM_0", "var_b0_HCCM_3", "var_b1_HCCM_3", "var_b2_HCCM_3"))

   
```
  
# Problem 2  
  
Now assume the model is homoskedastic. However, $\varepsilon_t=\rho\varepsilon_{t-1}+v_t$ and the $v_t$ are independent normal random variables with zero mean and variance $E[v_t^2]=\sigma_v^2$. Generate 100 samples of $y$, each of size 20 with $\rho=0.8$ and $\sigma_v^2=6.4$.  
  
## (2.a)  
  
Estimate the parameters using the least squares principle and provide their covariance matrix. Compare your results with the true parameters. What can you conclude?  
  
The OLS parameters are consistent with previous results, however, the vcov matrix is not correct due to autocorrelation. Without testing it would be difficult to know the nature of the problem. See table 4 for OLS parameter and vcov estimates.  
  
## (2.b)  
  
Test for the presence ofautocorrelation? What do you conclude?  
  
We can use the Durbin-Watson Test:  
  
The Hypotheses for the Durbin Watson test are:  
  
$H_0=$ no first order autocorrelation.  
  
$H_1=$ first order correlation exists.  
  
Assumptions are:  
  
That the errors are normally distributed with a mean of 0.  
  
The errors are stationary.  
  
\[DW = \frac{{\sum\limits_{t = 2}^T {{{({\varepsilon _t} - {\varepsilon _{t - 1}})}^2}} }}{{\sum\limits_{t = 1}^T {\varepsilon _t^2} }}\]  
  
where,  
  
$\varepsilon_t$ are residuals from an ordinary least squares regression.  
  
The Durbin Watson test reports a test statistic, with a value from 0 to 4, where:  
  
if $DW = 2$ then no autocorrelation.  
  
if $0 < DW < 2$ then positive autocorrelation (common in time series data).  
  
if $2 < DW < 4$ then negative autocorrelation (less common in time series data).  
  
Using the Durbin-Watson test (DW_Test) we obtain an average DW test statistic of 3.290 which indicate we do have autocorrelation that needs to be addressed. When we run the BP test (equation 2) we obtain a test statistic of 2.687 which indicates no problems with heteroskedasticity, as we already know since we constructed the data in this fashion.  
  
## (2.c)  
  
Assuming the errors follow AR(1) process, estimate the parameters using GLS and provide their covariance matrix.  
  
To obtain the parameters for the structure of the errors, in this case, AR(1), we can employ an FGLS method. In this situation we will use the technique developed by Cochrane and Orcutt (1949) which makes use of an iterative process to converge on the best fit for $\sigma^2$ and $\rho$. Once we have our error parameters we can construct $\Omega^{-1}$ and estimate our data parameters (beta's) and variance-covariance matrix, using FGLS.  The results are given in table 4 (below). We see that using the Cochrane–Orcutt method gives relatively accurate estimates of our $\hat\sigma^2$ and $\hat\rho$ parameters. Our true values of rho and sigma squared are 0.8 and 6.4 and we obtain an estimated of $\hat\rho=0.755$ and $\hat\sigma^2=6.055$. With a larger sample size I suspect our estimates would be more accurate but considering a sample size of 20, I feel that these estimates are impressive.  
  
Our beta estimates, using FGLS are a bit on the low side, again a possible situation of the stochastic nature of multiple experiments and low sample size. But the estimates are reasonable. The variance-covariance matrix is much tighter that the OLS estimates so we can see the FGLS procedure does a good job at correcting for the autocorrelation embedded in our data.  The t-values indicate that our parameters are significant.  
  
To help us better understand the distribution of the MLE parameter estimates over the range of the 100 experiments, I provide density plot's (below).  
  
```{r, echo=FALSE, message=FALSE, results='asis', fig.height=3, results='asis'}

stargazer(temp, header=F, type="latex", summary = T, font.size = "small", notes= c(), notes.align= "l", flip = F, float = T, float.env = "table", title="Summary Statistics: Problem 2, part (a), (b), and (c)", median = F)

```
  
```{r, echo=FALSE, message=FALSE, results='asis', fig.height=3, results='asis'}

par(mfrow=c(1,3))
plot(density(temp[,17]), main= expression(paste("FGLS-", beta[0], sep="")))
plot(density(temp[,18]), main= expression(paste("FGLS-", beta[1], sep="")))
plot(density(temp[,19]), main= expression(paste("FGLS-", beta[2], sep="")))
par(mfrow=c(1,2))
plot(density(temp[,15]), main= expression(paste("FGLS-", rho, sep="")))
plot(density(temp[,16]), main= expression(paste("FGLS-", sigma^2, sep="")))


```
  
```{r Problem 2 part d, echo=FALSE, message=FALSE, results='asis', fig.height=3, results='asis', warning=FALSE}


###################
### MLE Section ###
###################

### housekeeping ###

cat("\014")
rm(list=ls())
set.seed(123)

 library(readr)

data <- read_csv("~/Google Drive/digitalLibrary/*Econometrics3/Econometrics3/HW1_MatLab_Code/data.csv", col_names = FALSE)

n=20 # sample size
k=3 # number of paramaters in model
reps <- 10 # number of ols iterations
i <- 1 # counter
temp <- matrix(nrow = reps, ncol = 20) # matrix to stuff results into dim= "reps" X 3
x1 <- data$X2
x2 <- data$X3

### MLE functions ###

hw1.loglike <- function(params, y, x1, x2, y_t, y_t_1, x1_t, x2_t, x1_t_1, x2_t_1)
{
  beta1 <- params[1]
  beta2 <- params[2]
  beta3 <- params[3]
  rho <- params[4]
  sigma <- params[5]
  
  n <- length(x1)
  
lnL= (-0.5* log(2* pi)- 0.5* log(sigma^2)+ 0.5* log(1- rho^2)- ( ( (1- rho^2)* (y[1]- (beta1+ beta2* x1[1]+ beta3* x2[1]) )^2 ) / (sigma*2.5^2) ) ) + sum( -0.5* log(2* pi)- 0.5* log(sigma^2) - ( ( (y_t - rho* y_t_1)- ( 2* beta2* x1_t+ 2* rho* beta3* x2_t) )^2  / (2* sigma^2) ) )
  
  return(lnL)
}

hw1.mle <- function( par0, y, x1, x2, y_t, y_t_1, x1_t, x2_t, x1_t_1, x2_t_1)
{
  
   temp.mle <- optim(par0, hw1.loglike, y=y, x1=x1, x2=x2, y_t=y_t, y_t_1=y_t_1, x1_t=x1_t, x2_t=x2_t, x1_t_1=x1_t_1, x2_t_1=x2_t_1, method="Nelder-Mead", control=list(fnscale= -1, maxit = 30000, reltol=sqrt(.Machine$double.eps)), hessian= T)
  
   if(temp.mle$convergence!=0)
     stop("!!!DID NOT CONVERGE!!!")
  
  return(temp.mle)
}

### get starting values for MLE; standard OLS estimation ###

#for(i in 1:reps){
  
#e <- rnorm(n=n, mean=0, sd=5) # n-random normal draws for error term
#x1 <- rnorm(n=n, mean=100, sd=7) # n-random normal draws for x1 data
#x2 <- rnorm(n=n, mean=1000, sd=10) # n-random normal draws for x2 data

#e= sqrt(exp(-2+ 0.25* x1)) *rnorm(n)
et <- arima.sim(n = 20, list(ar = c(-0.8)), sd = sqrt(6.4))
et <- matrix(et)

y <- 10+ 1* x1+ 1* x2 + et # construct model

X <- matrix(c(rep(1, n), x1, x2), nrow=n, ncol=k) # gather X-data into single matrix

b_ols <- solve(crossprod(X, X)) %*% crossprod(X, y) # faster to use matrix algebra over lm function

##################################################################
### MLE estimation ###

par0= c(b_ols[1], b_ols[2], b_ols[3], 0.5, 0.5) # obtain starting values from ols
y_t = y[2:20]
y_t_1 = y[-2]
x1_t = x1[2:20]
x2_t = x2[2:20]
x1_t_1 = x1[-2]
x2_t_1 = x2[-2]
#hw1.loglike(params=par0, y=y, x1=x1, x2=x2)

beta_mle <- hw1.mle(par0=par0, y=y, x1=x1, x2=x2, y_t=y_t, y_t_1=y_t_1, x1_t=x1_t, x2_t=x2_t, x1_t_1=x1_t_1, x2_t_1=x2_t_1) # run MLE precedure

#beta_mle

#beta_mle1 <- nlm(hw1.loglike, p=par0, y=y, x1=x1, x2=x2) # run MLE precedure

beta_mle_params <- matrix(beta_mle$par)
hess <- beta_mle$hessian
#beta_mle_params
#var_mle <- (1)* diag(solve(beta_mle$hessian))
#var_mle
# dimnames(beta_mle_params) <- list(c(), c("betas_mle"))
# temp[i, 1] <- beta_mle_params[1]
# temp[i, 2] <- beta_mle_params[2]
# temp[i, 3] <- beta_mle_params[3] 
# temp[i, 4] <- beta_mle_params[4] 
# temp[i, 5] <- beta_mle_params[5] 
# temp[i, 6] <- var_mle[1]
# temp[i, 7] <- var_mle[2]
# temp[i, 8] <- var_mle[3]
# temp[i, 9] <- var_mle[4]
# temp[i, 10] <- var_mle[5]
# 
# temp[i, 11] <- sqrt(var_mle[1])
# temp[i, 12] <- sqrt(var_mle[2])
# temp[i, 13] <- sqrt(var_mle[3])
# temp[i, 14] <- sqrt(var_mle[4])
# temp[i, 15] <- sqrt(var_mle[5])
# 
# temp[i, 16] <- mean(beta_mle_params[1]/sqrt(var_mle[1]))
# temp[i, 17] <- mean(beta_mle_params[2]/sqrt(var_mle[2]))
# temp[i, 18] <- mean(beta_mle_params[3]/sqrt(var_mle[3]))
# temp[i, 19] <- mean(beta_mle_params[4]/sqrt(var_mle[4]))
# temp[i, 20] <- mean(beta_mle_params[5]/sqrt(var_mle[5]))

#mle_se <- sqrt(var_mle)

#mle_t_val <- beta_mle$par / mle_se
#mle_t_val <- matrix(mle_t_val, nrow=5)
#dimnames(mle_t_val) <- list(c(), c("t_values__mle"))
#mle_t_val
i <- i+1
#}

# dimnames(temp) <- list(c(), c("beta_0_mle", "beta_1_mle", "beta_2_mle", "alpha_0_mle", "alpha_1_mle", "var_b0_mle", "var_b1_mle", "var_b2_mle", "var_alpha_0_mle", "var_alpha_1_mle", "se_b0_mle", "se_b1_mle", "se_b2_mle", "se_alpha_0_mle", "se_alpha_1_mle", "t_val_b0_mle", "t_val_b1_mle", "t_val_b2_mle", "t_val_alpha_0_mle", "t_val_alpha_1_mle"))


```

## (2.d)  
  
Write a Matlab code to estimate the parameters using maximum likelihood method and provide their covariance matrix.
  
I am having a bit of trouble with convergence which suggests I have a formula problem with my lnL function. I can obtain estimates for about 60 sample iterations but then I have a problem with convergence.  
  
I will report the MLE parameter estimates for a single iteration and I will provide the Hessian matrix. I need to spend a bit more time with the raw code so that I can improve the convergence problems and work out the problems I have with the lnL formula...
  
\[\begin{gathered}
  \ln L = \left\{ { - 0.5\log (2\pi ) - 0.5\log ({\sigma _u}^2) + 0.5\log (1 - {\rho ^2}) - \frac{{(1 - {\rho ^2}){{({\beta _1} + {\beta _2}{x_1} + {\beta _3}{x_2})}^2}}}{{2{\sigma _u}^2}}} \right\} +  \hfill \\
  \sum\limits_{t = 2}^T {\left\{ { - 0.5\log (2\pi ) - 0.5\log ({\sigma _u}^2) - \frac{{{{[({y_t} - \rho {y_{t - 1}}) - ( - 2{\beta _2}{x_{1t - 1}} - 2\rho {\beta _3}{x_{2t - 1}})]}^2}}}{{2{\sigma _u}^2}}} \right\}}  \hfill \\ 
\end{gathered} \]

In the below tables are presented the MLE estimates for the betas the rho and sigma-squared parameters of the model with AR(1) innovations. We can clearly see that the parameter estimates are in the ball park but since I am only able to report a single experiment with only 20 observations the parameters are obviously not accurate. The second table shows the Hessian for the MLE.  
  
```{r, echo=FALSE, message=FALSE, results='asis', fig.height=3, results='asis'}

stargazer(beta_mle_params, hess, header=F, type="latex", summary = F, font.size = "small", notes= c(), notes.align= "l", flip = F, float = T, float.env = "table", title="Summary Statistics: Problem 2, part (d)", median = F)

```






